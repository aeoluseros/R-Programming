getSymbols("GOOG",src="oanda",from=from.date,to=to.date) #Current src methods available are: yahoo, google, MySQL, FRED, csv, RData, and oanda.
getSymbols("GOOG",src=" FRED",from=from.date,to=to.date) #Current src methods available are: yahoo, google, MySQL, FRED, csv, RData, and oanda.
getSymbols("GOOG",src="FRED",from=from.date,to=to.date) #Current src methods available are: yahoo, google, MySQL, FRED, csv, RData, and oanda.
getSymbols("GOOG",src="google",from=from.date,to=to.date) #Current src methods available are: yahoo, google, MySQL, FRED, csv, RData, and oanda.
head(GOOG)
class(GOOG)
getSymbols("AAPL",src="google",from=from.date,to=to.date) #Current src methods available are: yahoo, google, MySQL, FRED, csv, RData, and oanda.
head(AAPL)
mAAPL<-to.monthly(AAPL)
AAPLOpen<-Op(mAAPL)
as.matix(AAPL)
as.matRix(AAPL)
as.matrix(AAPL)
mAAPL<-to.monthly(as.matrix(AAPL))
mAAPL
GOOG<-as.matrix(GOOG)
mGoog<-to.monthly(GOOG[,-ncol(GOOG)])
googOpen<-Op(mGoog)
ts1<-ts(googOpen,frequency=12)
plot(ts1,xlab="Years+1",ylab="GOOG")
plot(decompose(ts1),xlab="Years+1")
?decompose
ts1Train<-window(ts1,start=1,end=5)
ts1Test<-window(ts1,start=5,end=(7-0.01))
ts1Train
ts1Test
ts1Train<-window(ts1,start=1,end=5)
ts1Test<-window(ts1,start=6,end=(7-0.01))
ts1Train
ts1Test
ts1Train<-window(ts1,start=1,end=5-0.01)
ts1Test<-window(ts1,start=5,end=(7-0.01))
ts1Train
ts1Test
plot(ts1Train)
lines(ma(ts1Train,order=3),col="red")
?,a
?ma
??ma
library(forecast)
lines(ma(ts1Train,order=3),col="red")
?ets
ets1<-ets(ts1Train,model="MMM")
fcast<-forecast(ets1)
plot(fcast)
lines(ts1Test,col="red")
ets1
names(ets1)
ets1$method
plot(fcast)
lines(ts1Test,col="red")
fcast
ets1
names(ets1)
ets1$method
ets1$initstate
?ets
ts1Train
fcast
?forecast
fcast<-forecast(ets1, fan=TRUE)
plot(fcast)
names(fcast)
facst$lower
fcast$lower
fcast$level
fcast<-forecast(ets1)
plot(fcast)
lines(ts1Test,col="red")
fcast$level
accuracy(fcast,ts1Test)
ts1Test
ts1Test<-window(ts1,start=5,end=(6-0.01))
ts1Test
plot(ts1Train)
library(forecast)
lines(ma(ts1Train,order=3),col="red")
ets1<-ets(ts1Train,model="MMM")
fcast<-forecast(ets1)
plot(fcast)
lines(ts1Test,col="red")
accuracy(fcast,ts1Test)
fcast
ts1Test
frequency(ets1)
fcast<-forecast(ets1, h = 1)
fcast
plot(fcast)
lines(ts1Test,col="red")
fcast<-forecast(ets1, h = 2)
fcast<-forecast(ets1, h = 12)
fcast
plot(fcast)
ts1Test<-window(ts1,start=5,end=(6-0.01))
frequency(ets1)
frequency(fcast)
frequency(ts1Test)
frequency(ts1Train)
data(iris);library(ggplot2)
inTrain<-createDataPartition(y=iris$Speices,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
dim(training);dim(testing)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
dim(training);dim(testing)
kMeans1<-kmeans(subset(training,select=-c(Species)),center=3)
training$clusters<-as.factor(kMeans$cluster)
training$clusters<-as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,color=clusters,data=training)
table(kMean1$cluster,training$Species)
table(kMeans1$cluster,training$Species)
modFit<-train(cluster~.,data=subset(training,select=-c(Species)),method="rpart")
subset(training,select=-c(Species))
modFit<-train(clusters~.,data=subset(training,select=-c(Species)),method="rpart")
table(predict(modFit,training),training$Species)
training
table(kMeans1$cluster,training$Species)
table(predict(modFit,training),training$Species)
testClusterPred<-predict(modFit,testing)
table(testClusterPred,testing$Species)
accuracy(testClusterPred,testing$Species)
?cl_predict
/?cl_predict
??cl_predict
library(clue)
install.packages("clue")
library(clue)
?cl_predict
data("Cassini")
nr <- NROW(Cassini$x)
ind <- sample(nr, 0.9 * nr, replace = FALSE)
party <- kmeans(Cassini$x[ind, ], 3)
table(cl_predict(party, Cassini$x[-ind, ]), Cassini$classes[-ind])
ind
nr
?NROW
nr <- nrow(Cassini$x)
ind <- sample(nr, 0.9 * nr, replace = FALSE)
party <- kmeans(Cassini$x[ind, ], 3)
table(cl_predict(party, Cassini$x[-ind, ]), Cassini$classes[-ind])
pbinom(2,size=500,prob=0.01)
ppois(2,lambda=500*0.01)
pbinom(2,size=1000,prob=0.01)
ppois(2,lambda=1000*0.01)
pbinom(2,size=1000,prob=0.0001)
ppois(2,lambda=1000*0.0001)
means<-cumsum(rnorm(n))/(1:n)
n<-1000
means<-cumsum(rnorm(n))/(1:n)
?cumsum
cumsum(rnorm(n))
means
plot(1:n,means)
abline(y=0)
abline(means=0)
abline(x,y=0)
?abline
abline(b=0)
abline(b=0)
abline(v=0)
abline(h=0)
means<-cumsum(rnorm(n))/(1:n)
plot(1:n,means)
abline(h=0)
n<-1000
means<-cumsum(rnorm(n))/(1:n)
plot(1:n,means)
abline(h=0)
means<-cumsum(sample(0:1,n,replace=TRUE))/(1:n)
means<-cumsum(sample(0:1,n,replace=TRUE))/(1:n)
plot(1:n,means)
abline(h=0)
abline(h=0.5)
0.56+c(-1,1)*qnorm(0.975)*sqrt(0.56*0.44/100)
binom.test(56,100)$conf.int
?binom.test
binom.test(56,1000)$conf.int
binom.test(56,100)
binom.test(560,1000)$conf.int
0.56+c(-1,1)*qnorm(0.975)*sqrt(0.56*0.44/100)
binom.test(560,1000)
binom.test(c(682, 243), p = 3/4)
binom.test(682, 682 + 243, p = 3/4)
binom.test(c(682, 243), p = 3/4)$conf.int
n<-20
pvals<-seq(0.1,0.9,by=0.05)
nosim<-1000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul<p)
})
coverage
?rbinom
rm(list=ls())
n<-20
pvals<-seq(0.1,0.9,by=0.05)
nosim<-1000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul<p)  #proporation of time that goes out of the lower and upper bound
})
plot(pvals,coverage)
?plot
plot(pvals,coverage,type="l")
abline(h=0.05)
n<-20
pvals<-seq(0.1,0.9,by=0.05)
nosim<-1000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of time that goes out of the lower and upper bound
})
plot(pvals,coverage,type="l")
abline(h=0.05)
abline(h=0.95)
nosim<-10000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
plot(pvals,coverage,type="l")
abline(h=0.95)
nosim<-1000
coverage
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
nosim<-100000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
plot(pvals,coverage,type="l")
abline(h=0.95)
coverage
nosim<-1000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
coverage
nosim<-1000000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
plot(pvals,coverage,type="l")
coverage
nosim<-1000
n<-100
pvals<-seq(0.1,0.9,by=0.05)
nosim<-1000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
plot(pvals,coverage,type="l")
abline(h=0.95)
n<-1000
pvals<-seq(0.1,0.9,by=0.05)
nosim<-1000
coverage<-sapply(pvals,function(p){
phats<-rbinom(nosim,prob=p,size=n)/n  #generate a 1000 of 10 coin flips
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
plot(pvals,coverage,type="l")
abline(h=0.95)
?rbinom
n<-20
coverage<-sapply(pvals,function(p){
phats<-(rbinom(nosim,prob=p,size=n)+2)/(n+4)
ll<-phats-qnorm(0.975)*sqrt(phats*(1-phats)/n)
ul<-phats+qnorm(0.975)*sqrt(phats*(1-phats)/n)
mean(ll<p & ul>p)  #proporation of times that insides the lower and upper bound
})
plot(pvals,coverage,type="l")
abline(h=0.95)
round(lambda+c(-1,1)*qnorm(0.975)*sqrt(lambda/t),3)
x<-5
t<-94.32
lambda<-x/t
round(lambda+c(-1,1)*qnorm(0.975)*sqrt(lambda/t),3)
poisson.test(x,T=t)$conf
poisson.test(x,T=t)$
poisson.test(x,T=t)
names(poisson.test(x,T=t))
poisson.test(x,T=t)$p.value
poisson.test(x,T=t)$method
poisson.test(x,T=t)$data.anem
poisson.test(x,T=t)$data
lambdavals<-seq(0.005,0.1,by=0.01)
nosim<-1000
t<-100
coverage<-sapply(lambdavals,function(lambda){
lhats<-rpois(nosim,lambda=lambda*t)/t
ll<-lhats-qnorm(0.975)*sqrt(lhats/t)
ul<-lhats+qnorm(0.975)*sqrt(lhats/t)
mean(ll<lambda & ul<lambda)
})
coverage
plot(pvals,coverage,type="l")
plot(lambdavals,coverage,type="l")
abline(h=0.95)
lambdavals<-seq(0.005,0.1,by=0.01)
nosim<-1000
t<-100
coverage<-sapply(lambdavals,function(lambda){
lhats<-rpois(nosim,lambda=lambda*t)/t
ll<-lhats-qnorm(0.975)*sqrt(lhats/t)
ul<-lhats+qnorm(0.975)*sqrt(lhats/t)
mean(ll<lambda & ul>lambda)
})
plot(lambdavals,coverage,type="l")  #gets really bad for small lambda values
abline(h=0.95)
t<-1000
coverage<-sapply(lambdavals,function(lambda){
lhats<-rpois(nosim,lambda=lambda*t)/t
ll<-lhats-qnorm(0.975)*sqrt(lhats/t)
ul<-lhats+qnorm(0.975)*sqrt(lhats/t)
mean(ll<lambda & ul>lambda)
})
plot(lambdavals,coverage,type="l")  #gets really bad for small lambda values
abline(h=0.95)
t<-1000
coverage<-sapply(lambdavals,function(lambda){
lhats<-rpois(nosim,lambda=lambda*t)/t
ll<-lhats-qnorm(0.975)*sqrt(lhats/t)
ul<-lhats+qnorm(0.975)*sqrt(lhats/t)
mean(ll<lambda & ul>lambda)
})
plot(lambdavals,coverage,type="l")  #gets really bad for small lambda values
abline(h=0.95)
t<-10000
coverage<-sapply(lambdavals,function(lambda){
lhats<-rpois(nosim,lambda=lambda*t)/t
ll<-lhats-qnorm(0.975)*sqrt(lhats/t)
ul<-lhats+qnorm(0.975)*sqrt(lhats/t)
mean(ll<lambda & ul>lambda)
})
plot(lambdavals,coverage,type="l")  #gets really bad for small lambda values
abline(h=0.95)
lhats<-rpois(nosim,lambda=lambda*t)/t
lhats<-rpois(nosim,lambda=lambda*t)/t
lhats
abline(0,1)
?abline
x <- mtcars$mpg
h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon", main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
x <- mtcars$mpg
h<-hist(x, breaks=10, col="red", xlab="Miles Per Gallon", main="Histogram with Normal Curve")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
plot(density(spam$your[spam$type=="nonspam"]),col="blue",main="",xlab="Frequency of 'your'")
library(kernlab)
data(spam)
plot(density(spam$your[spam$type=="nonspam"]),col="blue",main="",xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
library(caret);data(faithful);
head(faithful)
set.seed(333)
inTrain<-createDataPartition(y=faithful$waiting,p=0.75,list=FALSE)
trainFaith<-faithful[inTrain,]
testFaith<-faithful[-inTrain,]
head(trainFaith)
#fit a linear model
lm1<-lm(eruptions~waiting,data=trainFaith)
summary(lm1)
names(lm1)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,lm1$fitted.values,lwd=3)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)
#1. prediction with trees
lines(
lines(
library(ElemStatLearn);data(prostate)
str(prostate)
head(prostate)
covnames <- names(prostate[-(9:10)])
y <- prostate$lpsa
x <- prostate[,covnames]
form <- as.formula(paste("lpsa~", paste(covnames, collapse="+"), sep=""))
class(form)  #"formula"
summary(lm(form, data=prostate[prostate$train,]))
set.seed(1)
train.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)
y.test <- prostate$lpsa[-train.ind]
x.test <- x[-train.ind,]
y <- prostate$lpsa[train.ind]
x <- x[train.ind,]
p <- length(covnames)
rss <- list()   #create an empty list
for (i in 1:p) {
#cat(i)   #good idea!
Index <- combn(p,i)
#combn(x, m, FUN = NULL, simplify = TRUE, ...)
#x: vector source for combinations, or integer n for x <- seq_len(n).
#m: number of elements to choose
#cat(paste("index",Index))
rss[[i]] <- apply(Index, 2, function(is){  #apply by columns
form <- as.formula(paste("y~", paste(covnames[is], collapse="+"), sep=""))
#"lcavol"  "lweight" "age"     "lbph"    "svi"     "lcp"     "gleason" "pgg45"
cat(paste(form),"\n")
isfit <- lm(form, data=x)
yhat <- predict(isfit)  #if not newdata, don't have to specify data
train.rss <- sum((y - yhat)^2)
yhat <- predict(isfit, newdata=x.test)
test.rss <- sum((y.test - yhat)^2)
c(train.rss, test.rss)
})
}
#png("./PracticalMachLearning/selection-plots-01.png", height=432, width=432, pointsize=12)
plot(1:p, 1:p, type="n", ylim=range(unlist(rss)), xlim=c(0,p), xlab="number of predictors", ylab="residual sum of squares", main="Prostate cancer data")
for (i in 1:p) {
points(rep(i-0.15, ncol(rss[[i]])), rss[[i]][1, ], col="blue")
points(rep(i+0.15, ncol(rss[[i]])), rss[[i]][2, ], col="red")
}
minrss <- sapply(rss, function(x) min(x[1,]))
lines((1:p)-0.15, minrss, col="blue", lwd=1.7)
minrss <- sapply(rss, function(x) min(x[2,]))
lines((1:p)+0.15, minrss, col="red", lwd=1.7)
legend("topright", c("Train", "Test"), col=c("blue", "red"), pch=1)
plot(cars, main = "Stopping Distance versus Speed")
lines(stats::lowess(cars))
lowess(cars)
?lowess
data(sleep)
head(sleep)
library(ggplot2)
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
g
g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
names(sleep)
g1 <- sleep$extra[1 : 10]; g2 <- sleep$extra[11 : 20]
difference <- g2 - g1
mn <- mean(difference); s <- sd(difference); n <- 10
mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n)
t.test(difference)
t.test(g2, g1, paired = TRUE)
dim(sleep)
t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)
sleep
rbind(
mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n),
as.vector(t.test(difference)$conf.int),
as.vector(t.test(g2, g1, paired = TRUE)$conf.int),
as.vector(t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)$conf.int)
)
?as.vector
class(as.vector(t.test(difference)$conf.int))
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g + geom_line(size = 1, aes(colour = ID))
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g
g <- g + geom_line(size = 1, aes(colour = ID))
g + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
?ggplot
?relevel
sleep
I(relevel(group, 2))
?I
I(relevel(sleep$group, 2))
relevel(sleep$group, 2)
t.test(extra ~ relevel(group, 2), paired = TRUE, data = sleep)   #same as above
?qt
qt(.975, 9)
qt(1, n-1)
qt(1, 9)
qt(0.9, 9)
qt(-1, 9)
qt(-0.1, 9)
qt(0.1, 9)
qt(0, 9)
?t.test
rbind(
mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n),
as.vector(t.test(difference)$conf.int),      #class is numeric
as.vector(t.test(g2, g1, paired = TRUE)$conf.int),
as.vector(t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)$conf.int)
)
